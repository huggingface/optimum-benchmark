[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[project]
name = "optimum-benchmark"
dynamic = ["version"]
description = "Optimum-Benchmark is a unified multi-backend utility for benchmarking Transformers, Timm, Diffusers and Sentence-Transformers with full support of Optimum's hardware optimizations & quantization schemes."
readme = "README.md"
license = "Apache-2.0"
authors = [{ name = "HuggingFace Inc. Special Ops Team" }]
keywords = [
    "benchmark",
    "transformers",
    "quantization",
    "pruning",
    "optimization",
    "training",
    "inference",
    "onnx",
    "onnx runtime",
    "intel",
    "habana",
    "graphcore",
    "neural compressor",
    "ipex",
    "ipu",
    "hpu",
    "llm-swarm",
    "py-txi",
    "vllm",
    "llama-cpp",
    "gptqmodel",
    "sentence-transformers",
    "bitsandbytes",
    "codecarbon",
    "flash-attn",
    "deepspeed",
    "diffusers",
    "timm",
    "peft",
]
classifiers = [
    "Intended Audience :: Education",
    "Intended Audience :: Developers",
    "Operating System :: POSIX :: Linux",
    "Intended Audience :: Science/Research",
    "Programming Language :: Python :: 3.8",
    "Programming Language :: Python :: 3.9",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Topic :: Scientific/Engineering :: Artificial Intelligence",
]
requires-python = ">=3.10"
dependencies = [
    # HF dependencies
    "datasets>=4.0.0",
    "huggingface-hub",
    "transformers",
    "accelerate",
    "hf_xet",
    # Hydra
    "hydra-core",
    "omegaconf",
    # CPU
    "psutil",
    # Reporting
    "typing-extensions",
    "flatten_dict",
    "colorlog",
    "pandas",
    "rich",
    # GPU
    "nvidia-ml-py",
]

[project.urls]
Homepage = "https://github.com/huggingface/optimum-benchmark"
Repository = "https://github.com/huggingface/optimum-benchmark"
Issues = "https://github.com/huggingface/optimum-benchmark/issues"

[project.scripts]
optimum-benchmark = "optimum_benchmark.cli:main"

[project.optional-dependencies]
dev = [
    "ruff",
    "mock",
    "pytest",
    "codecarbon",
    "hydra-joblib-launcher",
    "torchcodec ; sys_platform != 'win32'",
    "peft; extra != 'tensorrt-llm' and extra != 'vllm'",
    "timm; extra != 'tensorrt-llm' and extra != 'vllm'",
    "librosa; extra != 'tensorrt-llm' and extra != 'vllm'",
    "diffusers; extra != 'tensorrt-llm' and extra != 'vllm'",
]
# optimum backends
ipex = ["optimum[ipex]>=1.27.0"]
openvino = ["optimum[openvino]>=1.27.0"]
onnxruntime = ["optimum[onnxruntime]>=1.27.0"]
onnxruntime-gpu = ["optimum[onnxruntime-gpu]>=1.27.0"]
# other backends
tensorrt-llm = [
    "huggingface-hub<0.26.0; extra != 'openvino' and extra != 'onnxruntime' and extra != 'onnxruntime-gpu' and extra != 'ipex' and extra != 'vllm' and extra != 'llama-cpp' and extra != 'py-txi'",
]
vllm = [
    "vllm; extra != 'openvino' and extra != 'onnxruntime' and extra != 'onnxruntime-gpu' and extra != 'ipex' and extra != 'tensorrt-llm' and extra != 'llama-cpp' and extra != 'py-txi'",
]
llama-cpp = ["llama-cpp-python"]
py-txi = ["py-txi"]
# optional dependencies
sentence-transformers = ["sentence-transformers"]
gptqmodel = ["gptqmodel", "optimum"]
bitsandbytes = ["bitsandbytes"]
codecarbon = ["codecarbon"]
flash-attn = ["flash-attn"]
diffusers = ["diffusers"]
deepspeed = ["deepspeed"]
torchao = ["torchao"]
timm = ["timm"]
peft = ["peft"]

[tool.hatch.version]
path = "optimum_benchmark/version.py"
pattern = '__version__ = "(?P<version>[^"]+)"'

[tool.hatch.build.targets.wheel]
packages = ["optimum_benchmark"]

[tool.ruff]
line-length = 120
lint.ignore = ["C901", "E501"]
lint.select = ["C", "E", "F", "I", "W", "I001"]

[tool.ruff.format]
line-ending = "auto"
quote-style = "double"
indent-style = "space"
skip-magic-trailing-comma = false

[tool.pytest.ini_options]
log_cli = true
log_cli_level = "INFO"
log_cli_date_format = "%Y-%m-%d %H:%M:%S"
log_cli_format = "[PYTEST-PROCESS][%(asctime)s][%(name)s][%(levelname)s] - %(message)s"

[tool.uv]
dev-dependencies = [
    "ruff",
    "mock",
    "pytest",
    "codecarbon",
    "hydra-joblib-launcher",
    "torchcodec ; sys_platform != 'win32'",
    "peft; extra != 'tensorrt-llm' and extra != 'vllm'",
    "timm; extra != 'tensorrt-llm' and extra != 'vllm'",
    "librosa; extra != 'tensorrt-llm' and extra != 'vllm'",
    "diffusers; extra != 'tensorrt-llm' and extra != 'vllm'",
]
