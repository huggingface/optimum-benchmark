defaults:
  - benchmark
  - scenario: inference
  - launcher: process
  - backend: vllm
  - _base_
  - _self_

name: cuda_vllm_llama

launcher:
  device_isolation: true
  device_isolation_action: warn

backend:
  device: cuda
  device_ids: 0
  no_weights: true
  serving_mode: online
  model: TinyLlama/TinyLlama-1.1B-Chat-v1.0
  engine_args:
    enforce_eager: true # remove for better perf but bigger memory footprint

scenario:
  input_shapes:
    batch_size: 1
    sequence_length: 64

  generate_kwargs:
    max_new_tokens: 32
    min_new_tokens: 32

hydra:
  job:
    env_set:
      VLLM_USE_V1: 0
